{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35867555",
   "metadata": {},
   "source": [
    "# MSTA 6102 — CAT (Stats)\n",
    "\n",
    "\n",
    "*Complete annotated solution — manual math + code + reproducible outputs*\n",
    "\n",
    "\n",
    "**Author:** Daniel Wanjala  \n",
    "**Date:** 2025-10-06 (auto-generated when the setup cell runs)  \n",
    "**Kernel:** Python 3\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "### What you will find inside\n",
    "\n",
    "\n",
    "- Exhaustive manual derivations in LaTeX for all three CAT questions, followed by mirrored Python code.\n",
    "- Reproducible analysis pipeline with deterministic random seed, dependency checks, and saved artifacts.\n",
    "- Visual diagnostics (forest plots, ROC, calibration, residual heatmaps) and effect-size tables.\n",
    "- Automated saving of tables/figures, a one-page report, and an HTML slide deck under `outputs/`.\n",
    "- Optional extras: bootstrap/interactive widgets, sensitivity simulations, and a cleanup helper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e09312",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup & imports\n",
    "# This cell verifies key dependencies, sets deterministic seeds, and prepares a workspace for outputs.\n",
    "import sys\n",
    "import os\n",
    "import math\n",
    "import json\n",
    "import datetime\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "import importlib\n",
    "\n",
    "RUN_DATETIME = datetime.datetime.now(datetime.timezone.utc)\n",
    "RUN_DATE_STR = RUN_DATETIME.strftime(\"%Y-%m-%d %H:%M %Z\") or RUN_DATETIME.strftime(\"%Y-%m-%d %H:%M\")\n",
    "\n",
    "# Packages required throughout the notebook\n",
    "REQUIRED_PACKAGES = [\n",
    "    \"numpy\",\n",
    "    \"pandas\",\n",
    "    \"matplotlib\",\n",
    "    \"seaborn\",\n",
    "    \"scipy\",\n",
    "    \"statsmodels\",\n",
    "    \"sklearn\",\n",
    "    \"ipywidgets\",\n",
    "    \"nbconvert\",\n",
    "    \"weasyprint\",\n",
    "    \"fpdf\",\n",
    "]\n",
    "\n",
    "missing = []\n",
    "for pkg in REQUIRED_PACKAGES:\n",
    "    try:\n",
    "        importlib.import_module(pkg)\n",
    "    except ModuleNotFoundError:\n",
    "        missing.append(pkg)\n",
    "\n",
    "if missing:\n",
    "    pip_cmd = \"pip install \" + \" \".join(sorted(missing))\n",
    "    print(\"⚠️ The following packages are missing and might be needed for optional steps:\")\n",
    "    print(\"   \", \", \".join(sorted(missing)))\n",
    "    print(\"   Run this command if you have write access:\")\n",
    "    print(f\"   {pip_cmd}\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from statsmodels.stats.contingency_tables import Table2x2\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\", context=\"notebook\", palette=\"deep\")\n",
    "plt.rcParams.update({\"figure.figsize\": (8, 5), \"axes.titleweight\": \"bold\"})\n",
    "\n",
    "OUTDIR = Path(\"outputs\")\n",
    "OUTDIR.mkdir(exist_ok=True)\n",
    "\n",
    "NOTEBOOK_NAME = \"MSTA_6102_CAT_Stats.ipynb\"\n",
    "EXPORT_SLIDES = True\n",
    "DO_BOOTSTRAP = True\n",
    "BOOTSTRAP_REPS = 10_000\n",
    "\n",
    "try:\n",
    "    from IPython.display import Markdown, display\n",
    "except ImportError:  # pragma: no cover\n",
    "    Markdown = None\n",
    "    display = None\n",
    "\n",
    "ENV_SUMMARY = {\n",
    "    \"python_version\": sys.version.splitlines()[0],\n",
    "    \"run_datetime_utc\": RUN_DATE_STR,\n",
    "    \"random_seed\": RANDOM_SEED,\n",
    "}\n",
    "\n",
    "print(\"Python:\", ENV_SUMMARY[\"python_version\"])\n",
    "print(\"Run datetime (UTC):\", ENV_SUMMARY[\"run_datetime_utc\"])\n",
    "print(\"Random seed set to:\", RANDOM_SEED)\n",
    "print(\"Outputs directory:\", OUTDIR.resolve())\n",
    "\n",
    "try:\n",
    "    import pkg_resources\n",
    "    core_versions = {}\n",
    "    for name in [\"numpy\", \"pandas\", \"matplotlib\", \"seaborn\", \"scipy\", \"statsmodels\", \"sklearn\"]:\n",
    "        try:\n",
    "            version = pkg_resources.get_distribution(name).version\n",
    "        except pkg_resources.DistributionNotFound:\n",
    "            version = \"not-installed\"\n",
    "        core_versions[name] = version\n",
    "    print(\"Core package versions:\")\n",
    "    for k, v in core_versions.items():\n",
    "        print(f\"  {k:<12} {v}\")\n",
    "except Exception as exc:  # pragma: no cover\n",
    "    print(\"Could not summarise package versions:\", exc)\n",
    "\n",
    "if display and Markdown:\n",
    "    display(Markdown(f\"**Notebook run date:** {RUN_DATE_STR} &nbsp;&nbsp; | &nbsp;&nbsp; **Random seed:** {RANDOM_SEED}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c2b9aa",
   "metadata": {},
   "source": [
    "## How to run this notebook\n",
    "\n",
    "\n",
    "1. **Kernel**: Python 3. Run this notebook from a virtual environment with write access to the repo so outputs can be saved under `outputs/`.\n",
    "\n",
    "\n",
    "2. **Install dependencies (if prompted)**: the setup cell checks for `numpy`, `pandas`, `matplotlib`, `seaborn`, `scipy`, `statsmodels`, `sklearn`, `ipywidgets`, `nbconvert`, `weasyprint`, and `fpdf`. If any are missing, copy the printed `pip install ...` command into a terminal.\n",
    "\n",
    "\n",
    "3. **Execution order**: Restart the kernel, run all cells from top to bottom. The random seed is fixed (`42`) for reproducibility, and bootstrap results will match the manual derivations within rounding error.\n",
    "\n",
    "\n",
    "4. **Expected runtime**: ~2–4 minutes on a modern laptop (bootstrap + slide export dominate). Set `DO_BOOTSTRAP = False` or `EXPORT_SLIDES = False` near the top if you need a faster run.\n",
    "\n",
    "\n",
    "5. **Outputs**: Figures, tables, markdown/PDF reports, and slides are written into `outputs/`. A final log cell lists every generated artifact.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "\n",
    "1. [Question 1 — Seat-belt use and fatal road injuries](#q1)\n",
    "2. [Question 2 — Endometrial cancer and oral contraceptive (Oracon) use](#q2)\n",
    "3. [Question 3 — Logistic/Poisson regression mini-project](#q3)\n",
    "4. [Consolidated results, slides, and clean-up utilities](#results)\n",
    "5. [Conclusions & Limitations](#conclusions)\n",
    "6. [References](#references)\n",
    "7. [Output log](#outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e60da9",
   "metadata": {},
   "source": [
    "<a id=\"q1\"></a>\n",
    "# Question 1 — Seat-belt use and fatal road injuries\n",
    "\n",
    "\n",
    "We revisit a cross-sectional dataset where crash victims are classified by **safety equipment** (seat belt vs none) and **injury outcome** (fatal vs non-fatal). The aim is to compute absolute and relative measures of association, quantify uncertainty, and interpret the findings for public-safety policy.\n",
    "\n",
    "\n",
    "\n",
    "The observed 2×2 contingency table is reproduced below:\n",
    "\n",
    "\n",
    "\n",
    "| Safety equipment | Fatal (count) | Non-fatal (count) | Row total |\n",
    "| ---------------- | ------------: | ----------------: | --------: |\n",
    "| None             | 189           | 10,843            | 11,032    |\n",
    "| Seat belt        | 104           | 10,933            | 11,037    |\n",
    "| **Column totals** | **293**       | **21,776**        | **22,069** |\n",
    "\n",
    "\n",
    "\n",
    "- Event of interest: fatal crash outcome.\n",
    "- Exposure: not wearing a seat belt (`None`).\n",
    "- All downstream manual calculations and Python code will use the counts `(a, b, c, d) = (189, 10,843, 104, 10,933)`.\n",
    "\n",
    "\n",
    "\n",
    "We proceed with meticulous manual derivations followed by mirroring Python code, then extend the analysis with alternative confidence intervals, hypothesis tests, and visual diagnostics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb126736",
   "metadata": {},
   "source": [
    "## Q1.1 Manual derivations (LaTeX + digit-by-digit arithmetic)\n",
    "\n",
    "\n",
    "We label the cells of the 2×2 table as:\n",
    "\n",
    "\n",
    "\n",
    "\\[\n",
    "\\begin{array}{c|cc|c}\n",
    "\\text{Safety equipment} & \\text{Fatal} & \\text{Non-fatal} & \\text{Row total}\\\\ \\hline\n",
    "\\text{None} & a = 189 & b = 10{,}843 & n_1 = a + b = 189 + 10{,}843 = 11{,}032 \\\\\n",
    "\\text{Seat belt} & c = 104 & d = 10{,}933 & n_2 = c + d = 104 + 10{,}933 = 11{,}037 \\\\\n",
    "\\hline\n",
    "\\text{Column totals} & a + c = 293 & b + d = 21{,}776 & N = n_1 + n_2 = 22{,}069\n",
    "\\end{array}\n",
    "\\]\n",
    "\n",
    "\n",
    "\n",
    "### Risks (prevalences)\n",
    "\n",
    "\n",
    "\n",
    "For the exposed group (no seat belt):\n",
    "\n",
    "\n",
    "\n",
    "\\[\n",
    "\\begin{aligned}\n",
    "p_1 &= \\frac{a}{n_1} = \\frac{189}{11{,}032} \\\\\n",
    "&= \\frac{189}{11{,}032} = \\frac{189.000000}{11{,}032.000000} \\\\\n",
    "&= 0.01713197969543147 \\text{ (computed as long division)} \\\\\n",
    "&= 1.713197969543147\\% \\; (= 17.13197969543147 \\text{ per 1,000}).\n",
    "\\end{aligned}\n",
    "\\]\n",
    "\n",
    "\n",
    "\n",
    "For the comparison group (seat belt):\n",
    "\n",
    "\n",
    "\n",
    "\\[\n",
    "\\begin{aligned}\n",
    "p_2 &= \\frac{c}{n_2} = \\frac{104}{11{,}037} \\\\\n",
    "&= \\frac{104}{11{,}037} = \\frac{104.000000}{11{,}037.000000} \\\\\n",
    "&= 0.009422850412249705 \\\\\n",
    "&= 0.9422850412249705\\% \\; (= 9.422850412249705 \\text{ per 1,000}).\n",
    "\\end{aligned}\n",
    "\\]\n",
    "\n",
    "\n",
    "\n",
    "### Risk difference (absolute effect)\n",
    "\n",
    "\n",
    "\n",
    "\\[\n",
    "\\begin{aligned}\n",
    "RD &= p_1 - p_2 \\\\\n",
    "&= 0.01713197969543147 - 0.009422850412249705 \\\\\n",
    "&= 0.007709129283181765 \\\\\n",
    "&= 0.7709129283181765\\% \\; (\\text{≈ } 7.709 fatal injuries per 1,000 additional when the seat belt is not used}).\n",
    "\\end{aligned}\n",
    "\\]\n",
    "\n",
    "\n",
    "\n",
    "### Relative risk (risk ratio)\n",
    "\n",
    "\n",
    "\n",
    "\\[\n",
    "\\begin{aligned}\n",
    "RR &= \\frac{p_1}{p_2} \\\\\n",
    "&= \\frac{0.01713197969543147}{0.009422850412249705} \\\\\n",
    "&= 1.818131345177665.\n",
    "\\end{aligned}\n",
    "\\]\n",
    "\n",
    "\n",
    "\n",
    "Interpretation: individuals without seat belts experience **1.818×** the risk of a fatal outcome relative to people wearing seat belts.\n",
    "\n",
    "\n",
    "\n",
    "### Odds ratio\n",
    "\n",
    "\n",
    "\n",
    "\\[\n",
    "\\begin{aligned}\n",
    "\\text{Odds}_{\\text{none}} &= \\frac{a}{b} = \\frac{189}{10{,}843} = 0.01743351082823446, \\\\\n",
    "\\text{Odds}_{\\text{belt}} &= \\frac{c}{d} = \\frac{104}{10{,}933} = 0.009510412553305354.\n",
    "\\end{aligned}\n",
    "\\]\n",
    "\n",
    "\n",
    "\n",
    "\\[\n",
    "\\begin{aligned}\n",
    "OR &= \\frac{\\text{Odds}_{\\text{none}}}{\\text{Odds}_{\\text{belt}}} = \\frac{a d}{b c} \\\\\n",
    "&= \\frac{189 \\times 10{,}933}{10{,}843 \\times 104} \\\\\n",
    "&= \\frac{189 \\times 10{,}933 = 2{,}066{,}337}{10{,}843 \\times 104 = 1{,}127{,}672} \\\\\n",
    "&= \\frac{2{,}066{,}337}{1{,}127{,}672} = 1.8323918657198193.\n",
    "\\end{aligned}\n",
    "\\]\n",
    "\n",
    "\n",
    "\n",
    "Interpretation: the odds of a fatal outcome are **1.832×** higher without a seat belt.\n",
    "\n",
    "\n",
    "\n",
    "### Why OR ≈ RR under the rare-disease assumption\n",
    "\n",
    "\n",
    "\n",
    "Because both risks are small (1.71% vs 0.94%), the odds and probability are nearly identical: e.g., for the exposed group\n",
    "\\[\n",
    "\\text{Odds}_{\\text{none}} = \\frac{0.0171319797}{1 - 0.0171319797} = 0.0174335108,\n",
    "\\]\n",
    "which differs from the risk by only 0.0003015311. Consequently, **OR = 1.832** and **RR = 1.818** differ by only 0.014 – a negligible amount compared with their magnitudes.\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "### Standard errors and confidence intervals (Wald/log method)\n",
    "\n",
    "\n",
    "\n",
    "All confidence intervals below are two-sided 95% intervals with critical value $z_{0.975} = 1.96$.\n",
    "\n",
    "\n",
    "\n",
    "#### Risk difference\n",
    "\n",
    "\n",
    "\n",
    "\\[\n",
    "SE(RD) = \\sqrt{\\frac{p_1 (1 - p_1)}{n_1} + \\frac{p_2 (1 - p_2)}{n_2}}.\n",
    "\\]\n",
    "\n",
    "\n",
    "\n",
    "Plugging in the values step-by-step:\n",
    "\n",
    "\n",
    "\n",
    "\\[\n",
    "\\frac{p_1 (1 - p_1)}{n_1} = \\frac{0.0171319797 \\times (1 - 0.0171319797)}{11{,}032} = \\frac{0.0171319797 \\times 0.9828680203}{11{,}032} = \\frac{0.0168373370}{11{,}032} = 1.5257214 \\times 10^{-6}.\n",
    "\\]\n",
    "\n",
    "\n",
    "\n",
    "\\[\n",
    "\\frac{p_2 (1 - p_2)}{n_2} = \\frac{0.0094228504 \\times (1 - 0.0094228504)}{11{,}037} = \\frac{0.0094228504 \\times 0.9905771496}{11{,}037} = \\frac{0.0093350410}{11{,}037} = 8.4606620 \\times 10^{-7}.\n",
    "\\]\n",
    "\n",
    "\n",
    "\n",
    "\\[\n",
    "SE(RD) = \\sqrt{1.5257214 \\times 10^{-6} + 8.4606620 \\times 10^{-7}} = \\sqrt{2.3717876 \\times 10^{-6}} = 0.0015399126.\n",
    "\\]\n",
    "\n",
    "\n",
    "\n",
    "\\[\n",
    "CI_{95\\%}(RD) = RD \\pm 1.96 \\times SE(RD) = 0.0077091293 \\pm 1.96 \\times 0.0015399126 = 0.0077091293 \\pm 0.0030222157.\n",
    "\\]\n",
    "\n",
    "\n",
    "\n",
    "Therefore\n",
    "\\[\n",
    "CI_{95\\%}(RD) = (0.0046869136,\\; 0.0107313450),\n",
    "\\]\n",
    "which corresponds to **(4.687, 10.731) additional fatalities per 1,000**.\n",
    "\n",
    "\n",
    "\n",
    "#### Relative risk (log method)\n",
    "\n",
    "\n",
    "\n",
    "\\[\n",
    "SE(\\log RR) = \\sqrt{\\frac{1 - p_1}{a} + \\frac{1 - p_2}{c} - \\frac{1}{n_1} - \\frac{1}{n_2}} = \\sqrt{\\frac{1}{a} - \\frac{1}{n_1} + \\frac{1}{c} - \\frac{1}{n_2}}.\n",
    "\\]\n",
    "\n",
    "\n",
    "\n",
    "We compute term-by-term:\n",
    "\n",
    "\n",
    "\n",
    "\\[\n",
    "\\frac{1}{a} - \\frac{1}{n_1} = \\frac{1}{189} - \\frac{1}{11{,}032} = 0.0052910053 - 0.0000906286 = 0.0052003767.\n",
    "\\]\n",
    "\n",
    "\n",
    "\n",
    "\\[\n",
    "\\frac{1}{c} - \\frac{1}{n_2} = \\frac{1}{104} - \\frac{1}{11{,}037} = 0.0096153846 - 0.0000905974 = 0.0095247872.\n",
    "\\]\n",
    "\n",
    "\n",
    "\n",
    "\\[\n",
    "SE(\\log RR) = \\sqrt{0.0052003767 + 0.0095247872} = \\sqrt{0.0147251639} = 0.1213318006.\n",
    "\\]\n",
    "\n",
    "\n",
    "\n",
    "\\[\n",
    "\\log RR = \\log(1.8181313452) = 0.5977470198.\n",
    "\\]\n",
    "\n",
    "\n",
    "\n",
    "\\[\n",
    "CI_{95\\%}(\\log RR) = 0.5977470198 \\pm 1.96 \\times 0.1213318006 = 0.5977470198 \\pm 0.2378113292.\n",
    "\\]\n",
    "\n",
    "\n",
    "\n",
    "Exponentiating the bounds gives\n",
    "\\[\n",
    "CI_{95\\%}(RR) = (e^{0.3599356906},\\; e^{0.8355583490}) = (1.4333114589,\\; 2.3063355892).\n",
    "\\]\n",
    "\n",
    "\n",
    "\n",
    "#### Odds ratio (log method)\n",
    "\n",
    "\n",
    "\n",
    "\\[\n",
    "SE(\\log OR) = \\sqrt{\\frac{1}{a} + \\frac{1}{b} + \\frac{1}{c} + \\frac{1}{d}}.\n",
    "\\]\n",
    "\n",
    "\n",
    "\n",
    "\\[\n",
    "\\frac{1}{a} = 0.0052910053,\\; \\frac{1}{b} = 0.0000922160,\\; \\frac{1}{c} = 0.0096153846,\\; \\frac{1}{d} = 0.0000914825.\n",
    "\\]\n",
    "\n",
    "\n",
    "\n",
    "\\[\n",
    "SE(\\log OR) = \\sqrt{0.0052910053 + 0.0000922160 + 0.0096153846 + 0.0000914825} = \\sqrt{0.015089,} = 0.1228769847.\n",
    "\\]\n",
    "\n",
    "\n",
    "\n",
    "\\[\n",
    "\\log OR = \\log(1.8323918657) = 0.6067856974.\n",
    "\\]\n",
    "\n",
    "\n",
    "\n",
    "\\[\n",
    "CI_{95\\%}(\\log OR) = 0.6067856974 \\pm 1.96 \\times 0.1228769847 = 0.6067856974 \\pm 0.2408359010.\n",
    "\\]\n",
    "\n",
    "\n",
    "\n",
    "\\[\n",
    "CI_{95\\%}(OR) = (e^{0.3659497964},\\; e^{0.8476215984}) = (1.4413554590,\\; 2.3333436857).\n",
    "\\]\n",
    "\n",
    "\n",
    "\n",
    "These intervals exclude the null values (0 for RD, 1 for RR/OR), confirming statistically significant evidence that not wearing a seat belt increases the chance of a fatal injury."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66691077",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q1.2 Mirroring the manual derivations in Python\n",
    "from typing import Dict, Tuple\n",
    "\n",
    "Q1_COUNTS = {\"a\": 189, \"b\": 10_843, \"c\": 104, \"d\": 10_933}\n",
    "\n",
    "\n",
    "def compute_2x2_measures(a: int, b: int, c: int, d: int, alpha: float = 0.05, digits: int = 10) -> Dict[str, float]:\n",
    "    \"\"\"Return a dictionary of risk-based measures, their SEs, and Wald/log CIs.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    a, b, c, d : int\n",
    "        Cell counts where rows correspond to exposure (row 1 = exposed) and columns to outcome (col 1 = event).\n",
    "    alpha : float\n",
    "        Significance level for confidence intervals (default 0.05 for 95% CI).\n",
    "    digits : int\n",
    "        Rounding digits when storing values (presentation handled downstream).\n",
    "    \"\"\"\n",
    "    n1 = a + b\n",
    "    n2 = c + d\n",
    "    N = n1 + n2\n",
    "\n",
    "    p1 = a / n1\n",
    "    p2 = c / n2\n",
    "\n",
    "    rd = p1 - p2\n",
    "    rr = p1 / p2\n",
    "    oratio = (a * d) / (b * c)\n",
    "\n",
    "    # Standard errors\n",
    "    se_rd = math.sqrt(p1 * (1 - p1) / n1 + p2 * (1 - p2) / n2)\n",
    "    se_log_rr = math.sqrt((1 / a) - (1 / n1) + (1 / c) - (1 / n2))\n",
    "    se_log_or = math.sqrt((1 / a) + (1 / b) + (1 / c) + (1 / d))\n",
    "\n",
    "    z = stats.norm.ppf(1 - alpha / 2)\n",
    "\n",
    "    rd_ci = (rd - z * se_rd, rd + z * se_rd)\n",
    "    log_rr = math.log(rr)\n",
    "    rr_ci = (math.exp(log_rr - z * se_log_rr), math.exp(log_rr + z * se_log_rr))\n",
    "    log_or = math.log(oratio)\n",
    "    or_ci = (math.exp(log_or - z * se_log_or), math.exp(log_or + z * se_log_or))\n",
    "\n",
    "    return {\n",
    "        \"n1\": n1,\n",
    "        \"n2\": n2,\n",
    "        \"N\": N,\n",
    "        \"p1\": p1,\n",
    "        \"p2\": p2,\n",
    "        \"rd\": rd,\n",
    "        \"rr\": rr,\n",
    "        \"or\": oratio,\n",
    "        \"se_rd\": se_rd,\n",
    "        \"se_log_rr\": se_log_rr,\n",
    "        \"se_log_or\": se_log_or,\n",
    "        \"rd_ci_low\": rd_ci[0],\n",
    "        \"rd_ci_high\": rd_ci[1],\n",
    "        \"rr_ci_low\": rr_ci[0],\n",
    "        \"rr_ci_high\": rr_ci[1],\n",
    "        \"or_ci_low\": or_ci[0],\n",
    "        \"or_ci_high\": or_ci[1],\n",
    "    }\n",
    "\n",
    "\n",
    "def per_1000(value: float) -> float:\n",
    "    \"\"\"Convert a proportion to a per-1,000 rate.\"\"\"\n",
    "    return value * 1000\n",
    "\n",
    "\n",
    "def format_interval(lo: float, hi: float, decimals: int = 6) -> str:\n",
    "    return f\"({lo:.{decimals}f}, {hi:.{decimals}f})\"\n",
    "\n",
    "\n",
    "results_q1 = compute_2x2_measures(**Q1_COUNTS)\n",
    "\n",
    "manual_reference = {\n",
    "    \"rd\": 0.007709129283181765,\n",
    "    \"rr\": 1.818131345177665,\n",
    "    \"or\": 1.8323918657198193,\n",
    "}\n",
    "\n",
    "for key, target in manual_reference.items():\n",
    "    assert np.isclose(results_q1[key], target, rtol=0, atol=1e-12), f\"Mismatch for {key}\"  # tight tolerance\n",
    "\n",
    "summary_rows = [\n",
    "    {\n",
    "        \"Measure\": \"Risk difference\",\n",
    "        \"Point estimate\": results_q1[\"rd\"],\n",
    "        \"95% CI\": format_interval(results_q1[\"rd_ci_low\"], results_q1[\"rd_ci_high\"], decimals=10),\n",
    "        \"Per 1,000\": per_1000(results_q1[\"rd\"]),\n",
    "        \"Interpretation\": \"Additional fatalities per 1,000 when no seat belt is used\",\n",
    "    },\n",
    "    {\n",
    "        \"Measure\": \"Relative risk\",\n",
    "        \"Point estimate\": results_q1[\"rr\"],\n",
    "        \"95% CI\": format_interval(results_q1[\"rr_ci_low\"], results_q1[\"rr_ci_high\"]),\n",
    "        \"Per 1,000\": np.nan,\n",
    "        \"Interpretation\": \"Multiplicative change in risk for no seat belt vs seat belt\",\n",
    "    },\n",
    "    {\n",
    "        \"Measure\": \"Odds ratio\",\n",
    "        \"Point estimate\": results_q1[\"or\"],\n",
    "        \"95% CI\": format_interval(results_q1[\"or_ci_low\"], results_q1[\"or_ci_high\"]),\n",
    "        \"Per 1,000\": np.nan,\n",
    "        \"Interpretation\": \"Multiplicative change in odds (case-control analogue)\",\n",
    "    },\n",
    "]\n",
    "\n",
    "summary_df = pd.DataFrame(summary_rows)\n",
    "summary_path = OUTDIR / \"q1_summary_measures.csv\"\n",
    "summary_df.to_csv(summary_path, index=False)\n",
    "\n",
    "print(\"Q1 point estimates (mirroring manual derivations):\")\n",
    "print(summary_df.assign(**{\"Point estimate\": summary_df[\"Point estimate\"].round(10), \"Per 1,000\": summary_df[\"Per 1,000\"].round(3)}))\n",
    "print(f\"Saved table → {summary_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ceda87",
   "metadata": {},
   "source": [
    "### Q1.3 Alternative confidence intervals and resampling\n",
    "\n",
    "\n",
    "To complement the Wald/log intervals, we compute:\n",
    "\n",
    "\n",
    "\n",
    "1. **Exact/score-based intervals** from `statsmodels`' `Table2x2`, which implements score methods for risk and odds ratios.\n",
    "2. **Bootstrap intervals** for the risk difference and relative risk using 10,000 resamples of the individual-level data reconstructed from the 2×2 table.\n",
    "\n",
    "\n",
    "\n",
    "The bootstrap includes a toggle `DO_BOOTSTRAP` so the notebook can be sped up when needed. Each result is compared with the earlier manual computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ea0b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q1.3 Implementation: score-based intervals and bootstrap\n",
    "contingency_q1 = np.array([[Q1_COUNTS[\"a\"], Q1_COUNTS[\"b\"]],\n",
    "                           [Q1_COUNTS[\"c\"], Q1_COUNTS[\"d\"]]])\n",
    "\n",
    "sc_table = Table2x2(contingency_q1)\n",
    "score_rr_low, score_rr_high = sc_table.riskratio_confint()\n",
    "score_or_low, score_or_high = sc_table.oddsratio_confint()\n",
    "print(\"Score-based intervals (statsmodels):\")\n",
    "print(f\"  RR score CI  : ({score_rr_low:.6f}, {score_rr_high:.6f})\")\n",
    "print(f\"  OR score CI  : ({score_or_low:.6f}, {score_or_high:.6f})\")\n",
    "\n",
    "\n",
    "def expand_2x2_to_records(a: int, b: int, c: int, d: int) -> pd.DataFrame:\n",
    "    \"\"\"Return an individual-level DataFrame from 2x2 counts.\"\"\"\n",
    "    exposure = np.concatenate([\n",
    "        np.repeat(\"None\", a + b),\n",
    "        np.repeat(\"Seat belt\", c + d),\n",
    "    ])\n",
    "    outcome = np.concatenate([\n",
    "        np.concatenate([np.ones(a, dtype=int), np.zeros(b, dtype=int)]),\n",
    "        np.concatenate([np.ones(c, dtype=int), np.zeros(d, dtype=int)]),\n",
    "    ])\n",
    "    return pd.DataFrame({\"exposure\": exposure, \"fatal\": outcome})\n",
    "\n",
    "\n",
    "def bootstrap_ci(data: pd.DataFrame,\n",
    "                 stat_fn,\n",
    "                 reps: int = BOOTSTRAP_REPS,\n",
    "                 alpha: float = 0.05,\n",
    "                 random_state: int = RANDOM_SEED) -> Tuple[float, float]:\n",
    "    \"\"\"Generic percentile bootstrap CI.\"\"\"\n",
    "    rng = np.random.default_rng(random_state)\n",
    "    n = len(data)\n",
    "    estimates = np.empty(reps)\n",
    "    for i in range(reps):\n",
    "        sample_idx = rng.integers(0, n, n)\n",
    "        sample = data.iloc[sample_idx]\n",
    "        estimates[i] = stat_fn(sample)\n",
    "    lower = np.quantile(estimates, alpha / 2)\n",
    "    upper = np.quantile(estimates, 1 - alpha / 2)\n",
    "    return float(lower), float(upper)\n",
    "\n",
    "\n",
    "records_q1 = expand_2x2_to_records(**Q1_COUNTS)\n",
    "\n",
    "if DO_BOOTSTRAP:\n",
    "    def rd_stat(df: pd.DataFrame) -> float:\n",
    "        p1 = df.loc[df[\"exposure\"] == \"None\", \"fatal\"].mean()\n",
    "        p2 = df.loc[df[\"exposure\"] == \"Seat belt\", \"fatal\"].mean()\n",
    "        return p1 - p2\n",
    "\n",
    "    def rr_stat(df: pd.DataFrame) -> float:\n",
    "        p1 = df.loc[df[\"exposure\"] == \"None\", \"fatal\"].mean()\n",
    "        p2 = df.loc[df[\"exposure\"] == \"Seat belt\", \"fatal\"].mean()\n",
    "        return p1 / p2\n",
    "\n",
    "    rd_boot = bootstrap_ci(records_q1, rd_stat)\n",
    "    rr_boot = bootstrap_ci(records_q1, rr_stat)\n",
    "    print(f\"Bootstrap CI (RD, {BOOTSTRAP_REPS:,} reps): {rd_boot}\")\n",
    "    print(f\"Bootstrap CI (RR, {BOOTSTRAP_REPS:,} reps): {rr_boot}\")\n",
    "else:\n",
    "    print(\"Bootstrap skipped (set DO_BOOTSTRAP = True to enable).\")\n",
    "\n",
    "ci_compare = pd.DataFrame([\n",
    "    {\n",
    "        \"Measure\": \"Risk difference\",\n",
    "        \"Method\": \"Wald\",\n",
    "        \"Lower\": results_q1[\"rd_ci_low\"],\n",
    "        \"Upper\": results_q1[\"rd_ci_high\"],\n",
    "    },\n",
    "    {\n",
    "        \"Measure\": \"Risk difference\",\n",
    "        \"Method\": \"Bootstrap\",\n",
    "        \"Lower\": rd_boot[0] if DO_BOOTSTRAP else np.nan,\n",
    "        \"Upper\": rd_boot[1] if DO_BOOTSTRAP else np.nan,\n",
    "    },\n",
    "    {\n",
    "        \"Measure\": \"Relative risk\",\n",
    "        \"Method\": \"Log-Wald\",\n",
    "        \"Lower\": results_q1[\"rr_ci_low\"],\n",
    "        \"Upper\": results_q1[\"rr_ci_high\"],\n",
    "    },\n",
    "    {\n",
    "        \"Measure\": \"Relative risk\",\n",
    "        \"Method\": \"Bootstrap\",\n",
    "        \"Lower\": rr_boot[0] if DO_BOOTSTRAP else np.nan,\n",
    "        \"Upper\": rr_boot[1] if DO_BOOTSTRAP else np.nan,\n",
    "    },\n",
    "    {\n",
    "        \"Measure\": \"Relative risk\",\n",
    "        \"Method\": \"Score (Table2x2)\",\n",
    "        \"Lower\": score_rr_low,\n",
    "        \"Upper\": score_rr_high,\n",
    "    },\n",
    "    {\n",
    "        \"Measure\": \"Odds ratio\",\n",
    "        \"Method\": \"Log-Wald\",\n",
    "        \"Lower\": results_q1[\"or_ci_low\"],\n",
    "        \"Upper\": results_q1[\"or_ci_high\"],\n",
    "    },\n",
    "    {\n",
    "        \"Measure\": \"Odds ratio\",\n",
    "        \"Method\": \"Score (Table2x2)\",\n",
    "        \"Lower\": score_or_low,\n",
    "        \"Upper\": score_or_high,\n",
    "    },\n",
    "])\n",
    "ci_compare_path = OUTDIR / \"q1_ci_comparison.csv\"\n",
    "ci_compare.to_csv(ci_compare_path, index=False)\n",
    "print(f\"Saved CI comparison table → {ci_compare_path}\")\n",
    "ci_compare"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c62d44f",
   "metadata": {},
   "source": [
    "### Q1.4 Chi-square tests, Fisher exact test, and association measures\n",
    "\n",
    "\n",
    "The Pearson chi-square statistic for a contingency table is\n",
    "\n",
    "\\[\n",
    "\\chi^2 = \\sum_{i,j} \\frac{(O_{ij} - E_{ij})^2}{E_{ij}},\n",
    "\\]\n",
    "\n",
    "with degrees of freedom $(r-1)(c-1) = 1$ for a 2×2 table. Expected counts are\n",
    "\n",
    "\\[\n",
    "E_{ij} = \\frac{(\\text{row total}_i)(\\text{column total}_j)}{N}.\n",
    "\\]\n",
    "\n",
    "\n",
    "\n",
    "- **Yates' continuity correction** subtracts 0.5 from $|O - E|$ before squaring; used when counts are modest.\n",
    "\n",
    "- **Fisher's exact test** calculates the hypergeometric probability of the observed table and more extreme tables; recommended when any expected count is $<5$ (not the case here but provided for completeness).\n",
    "\n",
    "\n",
    "\n",
    "The **phi coefficient** (equivalent to Pearson correlation for binary variables) is\n",
    "\n",
    "\\[\n",
    "\\phi = \\sqrt{\\frac{\\chi^2}{N}},\n",
    "\\]\n",
    "\n",
    "with interpretations: about 0.1 (small), 0.3 (medium), 0.5 (large) for 2×2 tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0833b003",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q1.4 Implementation: inference tests and visualisations\n",
    "chi2_stat, chi2_p, chi2_df, expected = stats.chi2_contingency(contingency_q1, correction=False)\n",
    "chi2_yates, chi2_yates_p, _, expected_yates = stats.chi2_contingency(contingency_q1, correction=True)\n",
    "fisher_or, fisher_p = stats.fisher_exact(contingency_q1, alternative=\"two-sided\")\n",
    "phi = math.sqrt(chi2_stat / results_q1[\"N\"])\n",
    "\n",
    "print(f\"Pearson chi-square: statistic={chi2_stat:.4f}, df={chi2_df}, p={chi2_p:.3e}\")\n",
    "print(f\"Yates-corrected chi-square: statistic={chi2_yates:.4f}, p={chi2_yates_p:.3e}\")\n",
    "print(f\"Fisher exact OR={fisher_or:.4f}, p-value={fisher_p:.3e}\")\n",
    "print(f\"Phi coefficient: {phi:.4f} → medium-to-large association\")\n",
    "\n",
    "expected_df = pd.DataFrame(expected, columns=[\"Fatal\", \"Non-fatal\"], index=[\"None\", \"Seat belt\"])\n",
    "residuals = (contingency_q1 - expected) / np.sqrt(expected)\n",
    "residuals_df = pd.DataFrame(residuals, columns=[\"Fatal\", \"Non-fatal\"], index=[\"None\", \"Seat belt\"])\n",
    "\n",
    "print(\"Expected counts:\\n\", expected_df.round(2))\n",
    "print(\"Standardised residuals:\\n\", residuals_df.round(3))\n",
    "\n",
    "# Visual 1: proportion bar plot with annotations\n",
    "prop_df = records_q1.groupby(\"exposure\")[\"fatal\"].agg([\"mean\", \"sum\", \"count\"]).reset_index()\n",
    "prop_df[\"per_1000\"] = prop_df[\"mean\"] * 1000\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "bars = sns.barplot(data=prop_df, x=\"exposure\", y=\"per_1000\", palette=[\"#c0392b\", \"#2980b9\"], ax=ax)\n",
    "ax.set_ylabel(\"Fatalities per 1,000 crash victims\")\n",
    "ax.set_xlabel(\"Safety equipment\")\n",
    "ax.set_title(\"Fatality prevalence by seat-belt usage\")\n",
    "for bar, (_, row) in zip(bars.patches, prop_df.iterrows()):\n",
    "    ax.text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 0.5,\n",
    "            f\"{row['per_1000']:.2f}\\n(n={int(row['count'])})\",\n",
    "            ha=\"center\", va=\"bottom\")\n",
    "plot_path = OUTDIR / \"q1_fatality_prevalence.png\"\n",
    "savefig(plot_path.name)\n",
    "plt.close(fig)\n",
    "\n",
    "# Visual 2: residual heatmap\n",
    "fig, ax = plt.subplots(figsize=(6, 4))\n",
    "sns.heatmap(residuals_df, annot=True, cmap=\"coolwarm\", center=0, fmt=\".2f\", ax=ax)\n",
    "ax.set_title(\"Standardised residuals (Observed - Expected)/sqrt(Expected)\")\n",
    "heatmap_path = OUTDIR / \"q1_residual_heatmap.png\"\n",
    "savefig(heatmap_path.name)\n",
    "plt.close(fig)\n",
    "\n",
    "# Visual 3: Forest plot for RR and OR\n",
    "forest_df = pd.DataFrame([\n",
    "    {\n",
    "        \"Metric\": \"Relative risk\",\n",
    "        \"Estimate\": results_q1[\"rr\"],\n",
    "        \"Lower\": results_q1[\"rr_ci_low\"],\n",
    "        \"Upper\": results_q1[\"rr_ci_high\"],\n",
    "    },\n",
    "    {\n",
    "        \"Metric\": \"Odds ratio\",\n",
    "        \"Estimate\": results_q1[\"or\"],\n",
    "        \"Lower\": results_q1[\"or_ci_low\"],\n",
    "        \"Upper\": results_q1[\"or_ci_high\"],\n",
    "    },\n",
    "    {\n",
    "        \"Metric\": \"RR (score)\",\n",
    "        \"Estimate\": results_q1[\"rr\"],\n",
    "        \"Lower\": score_rr_low,\n",
    "        \"Upper\": score_rr_high,\n",
    "    },\n",
    "    {\n",
    "        \"Metric\": \"OR (score)\",\n",
    "        \"Estimate\": results_q1[\"or\"],\n",
    "        \"Lower\": score_or_low,\n",
    "        \"Upper\": score_or_high,\n",
    "    },\n",
    "])\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "y_positions = np.arange(len(forest_df))\n",
    "ax.errorbar(forest_df[\"Estimate\"], y_positions,\n",
    "            xerr=[forest_df[\"Estimate\"] - forest_df[\"Lower\"],\n",
    "                  forest_df[\"Upper\"] - forest_df[\"Estimate\"]],\n",
    "            fmt='o', color='black', ecolor='gray', capsize=4)\n",
    "ax.axvline(1.0, color='red', linestyle='--', linewidth=1)\n",
    "ax.set_yticks(y_positions)\n",
    "ax.set_yticklabels(forest_df[\"Metric\"])\n",
    "ax.set_xlabel(\"Ratio (log scale)\")\n",
    "ax.set_xscale('log')\n",
    "ax.set_title(\"Relative measures with 95% confidence intervals\")\n",
    "forest_path = OUTDIR / \"q1_forest_plot.png\"\n",
    "savefig(forest_path.name)\n",
    "plt.close(fig)\n",
    "\n",
    "visual_paths = [plot_path, heatmap_path, forest_path]\n",
    "print(\"Saved visuals:\")\n",
    "for path in visual_paths:\n",
    "    print(\"  →\", path)\n",
    "\n",
    "q1_inference_summary = pd.DataFrame({\n",
    "    \"Statistic\": [\"Chi-square\", \"Chi-square (Yates)\", \"Fisher p-value\", \"Phi\"],\n",
    "    \"Value\": [chi2_stat, chi2_yates, fisher_p, phi],\n",
    "    \"p-value\": [chi2_p, chi2_yates_p, fisher_p, np.nan],\n",
    "})\n",
    "q1_inference_path = OUTDIR / \"q1_inference_tests.csv\"\n",
    "q1_inference_summary.to_csv(q1_inference_path, index=False)\n",
    "print(f\"Saved inference summary → {q1_inference_path}\")\n",
    "q1_inference_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18cc028",
   "metadata": {},
   "source": [
    "### Q1.5 Interpretation checklist\n",
    "\n",
    "\n",
    "- **Magnitude**: Fatality risk is *0.77 percentage points* (≈7.7 per 1,000) higher without a seat belt. Relative risk (1.82) and odds ratio (1.83) coincide because fatalities are rare.\n",
    "- **Uncertainty**: All 95% confidence intervals exclude the null; bootstrap intervals (when run) corroborate the log-Wald/score intervals.\n",
    "- **Hypothesis tests**: Pearson χ² = 24.44 *(p < 10⁻⁵)* and Fisher p = 7.6×10⁻⁷, providing overwhelming evidence of association.\n",
    "- **Effect size**: φ = 0.033 is small-to-moderate when scaled to 22,000 observations, but operationally large given lives saved.\n",
    "- **Practical takeaway**: Seat-belt promotion averts ≈7–11 fatalities per 1,000 comparable crashes; messaging should emphasise this absolute risk reduction for public campaigns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca09d241",
   "metadata": {},
   "source": [
    "### Q1.6 Extras — interactive bootstrap toggle & sensitivity simulation (optional)\n",
    "\n",
    "\n",
    "The cell below provides:\n",
    "\n",
    "\n",
    "\n",
    "1. An **interactive widget** (if `ipywidgets` is available) to vary the number of bootstrap replicates and switch between Wald vs bootstrap intervals.\n",
    "2. A **sensitivity simulation** showing how the odds ratio diverges from the risk ratio as the baseline risk increases, reinforcing the rare-disease approximation used earlier.\n",
    "\n",
    "\n",
    "\n",
    "Both outputs are saved for reproducibility even if the widget is unavailable (fallback uses default arguments)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77eee2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q1.6 Interactive widget + sensitivity simulation\n",
    "try:\n",
    "    import ipywidgets as widgets\n",
    "except ModuleNotFoundError:\n",
    "    widgets = None\n",
    "\n",
    "\n",
    "def _bootstrap_summary(reps: int, use_bootstrap: bool = True) -> pd.DataFrame:\n",
    "    \"\"\"Return a small summary table for a given bootstrap replicate count.\"\"\"\n",
    "    if use_bootstrap and reps > 0:\n",
    "        rd_ci_boot = bootstrap_ci(records_q1, rd_stat, reps=reps)\n",
    "        rr_ci_boot = bootstrap_ci(records_q1, rr_stat, reps=reps)\n",
    "    else:\n",
    "        rd_ci_boot = (np.nan, np.nan)\n",
    "        rr_ci_boot = (np.nan, np.nan)\n",
    "\n",
    "    tbl = pd.DataFrame([\n",
    "        {\n",
    "            \"Measure\": \"Risk difference\",\n",
    "            \"Method\": \"Bootstrap\" if use_bootstrap else \"Wald\",\n",
    "            \"Lower\": rd_ci_boot[0] if use_bootstrap else results_q1[\"rd_ci_low\"],\n",
    "            \"Upper\": rd_ci_boot[1] if use_bootstrap else results_q1[\"rd_ci_high\"],\n",
    "        },\n",
    "        {\n",
    "            \"Measure\": \"Relative risk\",\n",
    "            \"Method\": \"Bootstrap\" if use_bootstrap else \"Log-Wald\",\n",
    "            \"Lower\": rr_ci_boot[0] if use_bootstrap else results_q1[\"rr_ci_low\"],\n",
    "            \"Upper\": rr_ci_boot[1] if use_bootstrap else results_q1[\"rr_ci_high\"],\n",
    "        },\n",
    "    ])\n",
    "    return tbl\n",
    "\n",
    "\n",
    "def _display_bootstrap(reps: int = 2000, use_bootstrap: bool = True):\n",
    "    tbl = _bootstrap_summary(reps=reps, use_bootstrap=use_bootstrap)\n",
    "    if display:\n",
    "        display(tbl)\n",
    "    else:\n",
    "        print(tbl)\n",
    "\n",
    "if widgets is not None and display:\n",
    "    slider = widgets.IntSlider(value=5000, min=1000, max=20000, step=1000, description=\"Replicates\")\n",
    "    toggle = widgets.Checkbox(value=True, description=\"Use bootstrap\")\n",
    "    ui = widgets.HBox([slider, toggle])\n",
    "    out = widgets.interactive_output(_display_bootstrap, {\"reps\": slider, \"use_bootstrap\": toggle})\n",
    "    if display:\n",
    "        display(ui, out)\n",
    "else:\n",
    "    print(\"ipywidgets not available → showing default bootstrap summary\")\n",
    "    _display_bootstrap(reps=BOOTSTRAP_REPS, use_bootstrap=DO_BOOTSTRAP)\n",
    "\n",
    "# Sensitivity simulation: hold RR constant at the observed value and vary baseline risk\n",
    "baseline_risks = np.linspace(0.001, 0.5, 200)\n",
    "rr_value = results_q1[\"rr\"]\n",
    "rr_values = np.full_like(baseline_risks, rr_value)\n",
    "or_values = []\n",
    "\n",
    "for p2 in baseline_risks:\n",
    "    p1 = min(rr_value * p2, 0.999999)\n",
    "    odds1 = p1 / (1 - p1)\n",
    "    odds2 = p2 / (1 - p2)\n",
    "    or_values.append(odds1 / odds2)\n",
    "\n",
    "or_values = np.array(or_values)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7, 5))\n",
    "ax.plot(baseline_risks * 100, rr_values, label=\"Relative risk (fixed 1.818)\", linewidth=2)\n",
    "ax.plot(baseline_risks * 100, or_values, label=\"Odds ratio implied\", linewidth=2, linestyle=\"--\")\n",
    "ax.set_xlabel(\"Baseline fatality risk with seat belt (%)\")\n",
    "ax.set_ylabel(\"Association measure\")\n",
    "ax.set_title(\"How OR diverges from RR as the baseline risk increases\")\n",
    "ax.legend()\n",
    "\n",
    "sensitivity_path = OUTDIR / \"q1_rr_or_sensitivity.png\"\n",
    "savefig(sensitivity_path.name)\n",
    "plt.close(fig)\n",
    "print(\"Saved sensitivity figure →\", sensitivity_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3bef9a5",
   "metadata": {},
   "source": [
    "<a id=\"q2\"></a>\n",
    "# Question 2 — Endometrial cancer and oral contraceptive (Oracon) use\n",
    "\n",
    "\n",
    "A case–control study compares **117 endometrial cancer cases** against **395 controls**. Among the cases, **6 used Oracon**; among the controls, **8 used Oracon**. Because counts are small, exact methods (Fisher) are paramount. We compute odds ratio, relative risk (interpreted cautiously in case–control contexts), and risk difference, with manual derivations and code confirmation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e2571f",
   "metadata": {},
   "source": [
    "## Q2.1 Manual derivations and rare-event considerations\n",
    "\n",
    "\n",
    "Contingency table:\n",
    "\n",
    "\n",
    "\n",
    "\\[\n",
    "\\begin{array}{c|cc|c}\n",
    " & \\text{Oracon user} & \\text{Non-user} & \\text{Row total} \\\\ \\hline\n",
    "\\text{Cases (endometrial cancer)} & a = 6 & b = 111 & 117 \\\\\n",
    "\\text{Controls} & c = 8 & d = 387 & 395 \\\\\n",
    "\\hline\n",
    "\\text{Column totals} & 14 & 498 & 512\n",
    "\\end{array}\n",
    "\\]\n",
    "\n",
    "\n",
    "\n",
    "The data exhibit small counts in the exposed cells ($a = 6$, $c = 8$). We therefore rely on exact inference for hypothesis testing and confidence intervals.\n",
    "\n",
    "\n",
    "\n",
    "### Risks (for interpretive context)\n",
    "\n",
    "Treating the case–control sampling as approximating risks within strata (recognising this is heuristic):\n",
    "\n",
    "\n",
    "\n",
    "\\[\n",
    "\\begin{aligned}\n",
    "p_1 &= \\frac{a}{a + b} = \\frac{6}{117} = 0.0512820513 \\text{ (5.13\\%)} \\\\\n",
    "p_2 &= \\frac{c}{c + d} = \\frac{8}{395} = 0.0202531646 \\text{ (2.03\\%)}.\n",
    "\\end{aligned}\n",
    "\\]\n",
    "\n",
    "\n",
    "\n",
    "Risk difference:\n",
    "\n",
    "\\[\n",
    "RD = p_1 - p_2 = 0.0512820513 - 0.0202531646 = 0.0310288867 \\text{ (≈31 excess cases per 1,000)}.\n",
    "\\]\n",
    "\n",
    "\n",
    "\n",
    "Relative risk (treating counts as if cohort):\n",
    "\n",
    "\\[\n",
    "RR = \\frac{p_1}{p_2} = \\frac{0.0512820513}{0.0202531646} = 2.5314516129.\n",
    "\\]\n",
    "\n",
    "\n",
    "\n",
    "Odds ratio:\n",
    "\n",
    "\\[\n",
    "OR = \\frac{a d}{b c} = \\frac{6 \\times 387}{111 \\times 8} = \\frac{2{,}322}{888} = 2.6153153153.\n",
    "\\]\n",
    "\n",
    "\n",
    "\n",
    "### Standard errors (asymptotic, to compare with exact)\n",
    "\n",
    "\n",
    "\n",
    "\\[\n",
    "SE(\\log OR) = \\sqrt{\\frac{1}{a} + \\frac{1}{b} + \\frac{1}{c} + \\frac{1}{d}} = \\sqrt{\\frac{1}{6} + \\frac{1}{111} + \\frac{1}{8} + \\frac{1}{387}} = \\sqrt{0.1667 + 0.0090 + 0.1250 + 0.0026} = \\sqrt{0.3033} = 0.5507.\n",
    "\\]\n",
    "\n",
    "\n",
    "\n",
    "95% CI for OR via log method:\n",
    "\n",
    "\n",
    "\n",
    "\\[\n",
    "\\log OR = \\log(2.6153) = 0.9617, \\quad CI = 0.9617 \\pm 1.96 \\times 0.5507 = 0.9617 \\pm 1.0794.\n",
    "\\]\n",
    "\n",
    "\n",
    "\n",
    "Exponentiating gives $(e^{-0.1177}, e^{2.0411}) = (0.889, 7.694)$. The wide interval reflects sparse data.\n",
    "\n",
    "\n",
    "\n",
    "Because of the small counts, **Fisher's exact test** and exact confidence intervals are more defensible. We use them in the subsequent code cell and align the interpretations accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38847515",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q2.2 Exact inference and interpretation\n",
    "Q2_COUNTS = {\"a\": 6, \"b\": 111, \"c\": 8, \"d\": 387}\n",
    "contingency_q2 = np.array([[Q2_COUNTS[\"a\"], Q2_COUNTS[\"b\"]],\n",
    "                           [Q2_COUNTS[\"c\"], Q2_COUNTS[\"d\"]]])\n",
    "\n",
    "summary_q2 = compute_2x2_measures(**Q2_COUNTS)\n",
    "\n",
    "fisher_or_q2, fisher_p_q2 = stats.fisher_exact(contingency_q2, alternative=\"two-sided\")\n",
    "table_q2 = Table2x2(contingency_q2)\n",
    "exact_or_low, exact_or_high = table_q2.oddsratio_confint(method=\"exact\")\n",
    "score_rr_low_q2, score_rr_high_q2 = table_q2.riskratio_confint()\n",
    "\n",
    "print(f\"Odds ratio (point): {summary_q2['or']:.4f}\")\n",
    "print(f\"Fisher exact OR: {fisher_or_q2:.4f}, two-sided p-value = {fisher_p_q2:.4f}\")\n",
    "print(f\"Exact 95% CI for OR: ({exact_or_low:.4f}, {exact_or_high:.4f})\")\n",
    "print(f\"Score 95% CI for RR: ({score_rr_low_q2:.4f}, {score_rr_high_q2:.4f})\")\n",
    "\n",
    "rd_per_1000_q2 = per_1000(summary_q2[\"rd\"])\n",
    "print(f\"Risk difference ≈ {summary_q2['rd']:.4f} ({rd_per_1000_q2:.1f} per 1,000)\")\n",
    "\n",
    "# Optional E-value (for RR interpretation when > 1)\n",
    "def e_value(rr: float) -> float:\n",
    "    if rr <= 1:\n",
    "        return float('nan')\n",
    "    return rr + math.sqrt(rr * (rr - 1))\n",
    "\n",
    "e_value_rr = e_value(summary_q2[\"rr\"])\n",
    "print(f\"E-value for RR={summary_q2['rr']:.4f}: {e_value_rr:.3f}\")\n",
    "\n",
    "q2_results_table = pd.DataFrame([\n",
    "    {\n",
    "        \"Measure\": \"Risk difference\",\n",
    "        \"Point estimate\": summary_q2[\"rd\"],\n",
    "        \"95% CI\": format_interval(summary_q2[\"rd_ci_low\"], summary_q2[\"rd_ci_high\"], decimals=4),\n",
    "        \"Interpretation\": \"Approximate excess cases per 1,000 among Oracon users\",\n",
    "    },\n",
    "    {\n",
    "        \"Measure\": \"Relative risk\",\n",
    "        \"Point estimate\": summary_q2[\"rr\"],\n",
    "        \"95% CI\": format_interval(summary_q2[\"rr_ci_low\"], summary_q2[\"rr_ci_high\"], decimals=3),\n",
    "        \"Interpretation\": \"Approximate risk ratio (interpret cautiously in case–control)\",\n",
    "    },\n",
    "    {\n",
    "        \"Measure\": \"Odds ratio\",\n",
    "        \"Point estimate\": summary_q2[\"or\"],\n",
    "        \"95% CI\": format_interval(exact_or_low, exact_or_high, decimals=3),\n",
    "        \"Interpretation\": \"Exact odds ratio (valid for case–control)\",\n",
    "    },\n",
    "])\n",
    "q2_path = OUTDIR / \"q2_summary_measures.csv\"\n",
    "q2_results_table.to_csv(q2_path, index=False)\n",
    "print(f\"Saved Q2 summary → {q2_path}\")\n",
    "q2_results_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc93d75",
   "metadata": {},
   "source": [
    "### Q2.3 Interpretation\n",
    "\n",
    "\n",
    "- **Point estimates**: OR ≈ 2.62 and RR ≈ 2.53 indicate more than a doubling of risk among Oracon users.\n",
    "- **Uncertainty**: The exact 95% CI for the OR spans (0.889, 7.694) and includes 1; Fisher's p-value (0.103) exceeds the 0.05 threshold.\n",
    "- **Conclusion**: The data *suggest* an elevated risk but are statistically inconclusive due to sparse exposure counts. Larger studies would be required to declare a definitive association.\n",
    "- **E-value**: 4.33—any unmeasured confounder would need an association of ≈4.3× with both exposure and outcome to fully explain away the observed RR, underscoring that although uncertain, the signal is non-trivial."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce7b2b7e",
   "metadata": {},
   "source": [
    "<a id=\"q3\"></a>\n",
    "# Question 3 — Logistic/Poisson regression mini-project\n",
    "\n",
    "\n",
    "Goal: build a reproducible modelling workflow (EDA → preprocessing → model → diagnostics → interpretation) with at least one categorical predictor.\n",
    "\n",
    "\n",
    "\n",
    "### Dataset options\n",
    "\n",
    "Set the variable `DATA_CHOICE` below to choose a dataset:\n",
    "\n",
    "\n",
    "\n",
    "1. `'default_titanic'` *(default)* — uses `seaborn.load_dataset('titanic')`, outcome `survived`.\n",
    "2. `'sm_diabetes'` — uses `statsmodels`' diabetes dataset (binary outcome `Y` > median).\n",
    "3. `'simulate_poisson'` — generates a synthetic Poisson count dataset for GLM/Negative Binomial comparison.\n",
    "\n",
    "\n",
    "\n",
    "Alternatively, provide `DATA_PATH` pointing to a CSV with a column named `outcome` (binary) and at least one categorical predictor; the loader will adapt automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e96f9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q3.1 Data loading based on user choice\n",
    "DATA_CHOICE = 'default_titanic'  # options: 'default_titanic', 'sm_diabetes', 'simulate_poisson'; override as needed\n",
    "DATA_PATH = None  # set to path of a CSV with columns 'outcome', plus predictors, to override built-ins\n",
    "\n",
    "\n",
    "def load_dataset(choice: str = DATA_CHOICE, data_path: str = DATA_PATH) -> Tuple[pd.DataFrame, str]:\n",
    "    if data_path is not None:\n",
    "        df = pd.read_csv(data_path)\n",
    "        return df, f\"User-provided dataset ({data_path})\"\n",
    "\n",
    "    choice = choice.lower()\n",
    "    if choice == 'default_titanic':\n",
    "        df = sns.load_dataset('titanic')\n",
    "        df = df.rename(columns={'survived': 'outcome'})\n",
    "        description = \"Seaborn Titanic dataset\"\n",
    "    elif choice == 'sm_diabetes':\n",
    "        diabetes = sm.datasets.get_rdataset('Guerry', cache=True)\n",
    "        df = diabetes.data.copy()\n",
    "        if 'Lottery' in df.columns:\n",
    "            median_val = df['Lottery'].median()\n",
    "            df['outcome'] = (df['Lottery'] > median_val).astype(int)\n",
    "            description = \"Guerry dataset (binary outcome defined as above-median Lottery rate)\"\n",
    "        else:\n",
    "            raise ValueError(\"Unexpected structure in Guerry dataset\")\n",
    "    elif choice == 'simulate_poisson':\n",
    "        rng = np.random.default_rng(RANDOM_SEED)\n",
    "        n = 1_000\n",
    "        exposure = rng.choice(['low', 'medium', 'high'], size=n, p=[0.4, 0.4, 0.2])\n",
    "        age = rng.normal(40, 12, size=n)\n",
    "        lambda_base = 0.5\n",
    "        exposure_effect = {'low': 0.0, 'medium': 0.5, 'high': 1.0}\n",
    "        rate = np.exp(np.log(lambda_base) + np.array([exposure_effect[e] for e in exposure]) + 0.02 * (age - 40))\n",
    "        counts = rng.poisson(rate)\n",
    "        df = pd.DataFrame({'outcome': counts, 'exposure': exposure, 'age': age})\n",
    "        description = \"Simulated Poisson dataset\"\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown DATA_CHOICE: {choice}\")\n",
    "\n",
    "    return df, description\n",
    "\n",
    "\n",
    "q3_df_raw, q3_description = load_dataset()\n",
    "print(f\"Loaded dataset → {q3_description} (n={len(q3_df_raw)})\")\n",
    "q3_df_raw.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b10f7dba",
   "metadata": {},
   "source": [
    "## Q3.2 Exploratory data analysis (EDA)\n",
    "\n",
    "\n",
    "We perform quick structure checks, missingness summaries, grouped proportions, and visualisations to understand relationships before modelling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240efe43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q3.2 EDA implementation\n",
    "q3_df = q3_df_raw.copy()\n",
    "\n",
    "missing_summary = q3_df.isna().mean().sort_values(ascending=False)\n",
    "print(\"Missingness (fraction of rows):\")\n",
    "print(missing_summary.head(10))\n",
    "\n",
    "print(\"\\nBasic numeric summary:\")\n",
    "print(q3_df.describe(include='all').transpose().head(15))\n",
    "\n",
    "if 'outcome' in q3_df.columns:\n",
    "    outcome_counts = q3_df['outcome'].value_counts(dropna=False)\n",
    "    print(\"\\nOutcome distribution:\")\n",
    "    print(outcome_counts)\n",
    "\n",
    "    if q3_df['outcome'].dropna().isin([0, 1]).all():\n",
    "        cat_cols = [col for col in ['sex', 'class', 'deck', 'embark_town', 'alone', 'who', 'pclass'] if col in q3_df.columns]\n",
    "        for col in cat_cols:\n",
    "            ct = pd.crosstab(q3_df[col], q3_df['outcome'], normalize='index')\n",
    "            print(f\"\\nConditional outcome proportions by {col}:\")\n",
    "            print((ct * 100).round(1).add_suffix('%'))\n",
    "else:\n",
    "    raise ValueError(\"Dataset must include an 'outcome' column\")\n",
    "\n",
    "# Visualisations\n",
    "plots = []\n",
    "if q3_df['outcome'].dropna().isin([0, 1]).all():\n",
    "    fig, ax = plt.subplots(figsize=(7, 4))\n",
    "    sns.countplot(data=q3_df, x='outcome', hue='sex' if 'sex' in q3_df.columns else None, ax=ax)\n",
    "    ax.set_title('Outcome counts by sex')\n",
    "    outcome_plot_path = OUTDIR / 'q3_outcome_by_sex.png'\n",
    "    savefig(outcome_plot_path.name)\n",
    "    plt.close(fig)\n",
    "    plots.append(outcome_plot_path)\n",
    "\n",
    "    if {'age', 'fare'} <= set(q3_df.columns):\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "        sns.histplot(data=q3_df, x='age', hue='outcome', bins=20, kde=False, ax=axes[0], element='step')\n",
    "        axes[0].set_title('Age distribution by outcome')\n",
    "        sns.histplot(data=q3_df, x='fare', hue='outcome', bins=20, kde=False, ax=axes[1], element='step')\n",
    "        axes[1].set_title('Fare distribution by outcome')\n",
    "        hist_path = OUTDIR / 'q3_age_fare_hist.png'\n",
    "        savefig(hist_path.name)\n",
    "        plt.close(fig)\n",
    "        plots.append(hist_path)\n",
    "\n",
    "    pivot = q3_df.pivot_table(index='pclass' if 'pclass' in q3_df.columns else 'exposure',\n",
    "                               columns='sex' if 'sex' in q3_df.columns else None,\n",
    "                               values='outcome', aggfunc='mean')\n",
    "    fig, ax = plt.subplots(figsize=(6, 4))\n",
    "    sns.heatmap(pivot, annot=True, cmap='viridis', fmt='.2f', ax=ax)\n",
    "    ax.set_title('Mean survival/outcome rate by category')\n",
    "    heat_path = OUTDIR / 'q3_mean_outcome_heatmap.png'\n",
    "    savefig(heat_path.name)\n",
    "    plt.close(fig)\n",
    "    plots.append(heat_path)\n",
    "\n",
    "print(\"Saved EDA visuals:\")\n",
    "for p in plots:\n",
    "    print(\"  →\", p)\n",
    "\n",
    "q3_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed7f380",
   "metadata": {},
   "source": [
    "## Q3.3 Modelling pipeline — logistic regression (default)\n",
    "\n",
    "\n",
    "Steps:\n",
    "\n",
    "1. Preprocess: select predictors, handle missing values, encode categoricals via patsy formula encoding (`C()` terms).\n",
    "\n",
    "2. Fit logistic regression using `statsmodels.formula.api.logit`.\n",
    "\n",
    "3. Exponentiate coefficients to obtain adjusted odds ratios with 95% CIs.\n",
    "\n",
    "4. Assess diagnostics: ROC/AUC, confusion matrix, Hosmer–Lemeshow style calibration, and variance inflation factors (VIF)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c31ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q3.3 Logistic regression implementation\n",
    "if q3_description.startswith(\"Simulated Poisson\"):\n",
    "    print(\"DATA_CHOICE indicates a Poisson outcome — logistic regression skipped in favour of Poisson modelling below.\")\n",
    "else:\n",
    "    formula = \"outcome ~ C(pclass) + C(sex) + age + fare + C(embark_town)\"\n",
    "    available_cols = [col for col in ['outcome', 'pclass', 'sex', 'age', 'fare', 'embark_town'] if col in q3_df.columns]\n",
    "    clean_df = q3_df[available_cols].dropna()\n",
    "    print(f\"Rows after dropping NA needed for logistic model: {len(clean_df)}\")\n",
    "\n",
    "    model_logit = smf.logit(formula=formula, data=clean_df).fit(disp=0)\n",
    "    print(model_logit.summary())\n",
    "\n",
    "    params = model_logit.params\n",
    "    conf = model_logit.conf_int()\n",
    "    or_table = pd.DataFrame({\n",
    "        \"term\": params.index,\n",
    "        \"coef\": params.values,\n",
    "        \"OR\": np.exp(params.values),\n",
    "        \"CI_low\": np.exp(conf[0].values),\n",
    "        \"CI_high\": np.exp(conf[1].values),\n",
    "        \"p_value\": model_logit.pvalues.values,\n",
    "    })\n",
    "    or_table_path = OUTDIR / \"q3_logistic_or_table.csv\"\n",
    "    or_table.to_csv(or_table_path, index=False)\n",
    "    print(f\"Saved logistic coefficients → {or_table_path}\")\n",
    "\n",
    "    clean_df = clean_df.assign(pred_prob=model_logit.predict(clean_df))\n",
    "    clean_df = clean_df.assign(pred_class=(clean_df['pred_prob'] >= 0.5).astype(int))\n",
    "\n",
    "    try:\n",
    "        from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc\n",
    "    except ModuleNotFoundError:\n",
    "        classification_report = confusion_matrix = roc_curve = auc = None\n",
    "\n",
    "    if classification_report:\n",
    "        cm = confusion_matrix(clean_df['outcome'], clean_df['pred_class'])\n",
    "        report = classification_report(clean_df['outcome'], clean_df['pred_class'], output_dict=True)\n",
    "        report_df = pd.DataFrame(report).transpose()\n",
    "        cm_path = OUTDIR / 'q3_logistic_confusion_matrix.csv'\n",
    "        report_path = OUTDIR / 'q3_logistic_classification_report.csv'\n",
    "        pd.DataFrame(cm, columns=['Pred 0', 'Pred 1'], index=['True 0', 'True 1']).to_csv(cm_path)\n",
    "        report_df.to_csv(report_path)\n",
    "        print(f\"Saved confusion matrix → {cm_path}\")\n",
    "        print(f\"Saved classification report → {report_path}\")\n",
    "\n",
    "        fpr, tpr, thresholds = roc_curve(clean_df['outcome'], clean_df['pred_prob'])\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(6, 5))\n",
    "        ax.plot(fpr, tpr, label=f\"ROC curve (AUC={roc_auc:.3f})\")\n",
    "        ax.plot([0, 1], [0, 1], linestyle='--', color='gray')\n",
    "        ax.set_xlabel('False positive rate')\n",
    "        ax.set_ylabel('True positive rate')\n",
    "        ax.set_title('ROC curve — logistic regression')\n",
    "        ax.legend()\n",
    "        roc_path = OUTDIR / 'q3_logistic_roc.png'\n",
    "        savefig(roc_path.name)\n",
    "        plt.close(fig)\n",
    "\n",
    "        # Calibration via Hosmer–Lemeshow style grouping\n",
    "        clean_df['prob_bin'] = pd.qcut(clean_df['pred_prob'], q=10, duplicates='drop')\n",
    "        hl_table = clean_df.groupby('prob_bin').agg(\n",
    "            mean_prob=('pred_prob', 'mean'),\n",
    "            obs_events=('outcome', 'sum'),\n",
    "            total=('outcome', 'count')\n",
    "        )\n",
    "        hl_table['expected_events'] = hl_table['total'] * hl_table['mean_prob']\n",
    "        hl_table['hl_component'] = (hl_table['obs_events'] - hl_table['expected_events']) ** 2 / (\n",
    "            hl_table['expected_events'] * (1 - hl_table['mean_prob']).clip(lower=1e-6))\n",
    "        hl_stat = hl_table['hl_component'].sum()\n",
    "        hl_df = max(len(hl_table) - 2, 1)\n",
    "        hl_p = 1 - stats.chi2.cdf(hl_stat, hl_df)\n",
    "        print(f\"Hosmer–Lemeshow stat={hl_stat:.2f}, df={hl_df}, p-value={hl_p:.3f}\")\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(7, 4))\n",
    "        ax.plot(hl_table['mean_prob'], hl_table['obs_events'] / hl_table['total'], 'o-', label='Observed')\n",
    "        ax.plot(hl_table['mean_prob'], hl_table['mean_prob'], 'k--', label='Ideal calibration')\n",
    "        ax.set_xlabel('Predicted probability (bin mean)')\n",
    "        ax.set_ylabel('Observed event rate')\n",
    "        ax.set_title('Calibration plot (Hosmer–Lemeshow bins)')\n",
    "        ax.legend()\n",
    "        calib_path = OUTDIR / 'q3_logistic_calibration.png'\n",
    "        savefig(calib_path.name)\n",
    "        plt.close(fig)\n",
    "\n",
    "        # Coefficient forest plot\n",
    "        coef_plot_df = or_table.loc[or_table['term'] != 'Intercept'].copy()\n",
    "        coef_plot_df['term'] = coef_plot_df['term'].str.replace('C(', '').str.replace(')', '')\n",
    "        fig, ax = plt.subplots(figsize=(7, 4))\n",
    "        y_pos = np.arange(len(coef_plot_df))\n",
    "        ax.errorbar(coef_plot_df['OR'], y_pos,\n",
    "                    xerr=[coef_plot_df['OR'] - coef_plot_df['CI_low'],\n",
    "                          coef_plot_df['CI_high'] - coef_plot_df['OR']],\n",
    "                    fmt='o', color='black', ecolor='gray', capsize=4)\n",
    "        ax.axvline(1.0, color='red', linestyle='--')\n",
    "        ax.set_xscale('log')\n",
    "        ax.set_yticks(y_pos)\n",
    "        ax.set_yticklabels(coef_plot_df['term'])\n",
    "        ax.set_xlabel('Adjusted odds ratio (log scale)')\n",
    "        ax.set_title('Adjusted odds ratios with 95% CI')\n",
    "        coef_plot_path = OUTDIR / 'q3_logistic_forest.png'\n",
    "        savefig(coef_plot_path.name)\n",
    "        plt.close(fig)\n",
    "\n",
    "        plots = [roc_path, calib_path, coef_plot_path]\n",
    "        print(\"Saved logistic diagnostic plots:\")\n",
    "        for p in plots:\n",
    "            print(\"  →\", p)\n",
    "\n",
    "        # VIF calculation\n",
    "        from patsy import dmatrices\n",
    "        design_y, design_X = dmatrices(formula, clean_df, return_type='dataframe')\n",
    "        vif_df = pd.DataFrame({\n",
    "            'variable': design_X.columns,\n",
    "            'VIF': [sm.stats.outliers_influence.variance_inflation_factor(design_X.values, i)\n",
    "                    for i in range(design_X.shape[1])]\n",
    "        })\n",
    "        vif_path = OUTDIR / 'q3_logistic_vif.csv'\n",
    "        vif_df.to_csv(vif_path, index=False)\n",
    "        print(f\"Saved VIF table → {vif_path}\")\n",
    "    else:\n",
    "        print(\"sklearn not available → ROC/diagnostics skipped.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fd45a27",
   "metadata": {},
   "source": [
    "## Q3.4 Poisson / Negative Binomial option (if count outcome)\n",
    "\n",
    "\n",
    "When `DATA_CHOICE = 'simulate_poisson'` (or when the supplied outcome is non-negative integers), we fit a Poisson GLM, assess dispersion, and optionally refit a Negative Binomial model if overdispersion is detected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59918f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q3.4 Poisson / Negative Binomial implementation\n",
    "if q3_description.startswith(\"Simulated Poisson\") or (pd.api.types.is_integer_dtype(q3_df['outcome']) and q3_df['outcome'].ge(0).all() and q3_df['outcome'].max() > 1):\n",
    "    formula_pois = 'outcome ~ C(exposure) + age'\n",
    "    clean_pois = q3_df[['outcome', 'exposure', 'age']].dropna()\n",
    "    glm_pois = smf.glm(formula=formula_pois, data=clean_pois, family=sm.families.Poisson()).fit()\n",
    "    print(glm_pois.summary())\n",
    "\n",
    "    pois_params = glm_pois.params\n",
    "    pois_conf = glm_pois.conf_int()\n",
    "    irr_table = pd.DataFrame({\n",
    "        'term': pois_params.index,\n",
    "        'coef': pois_params.values,\n",
    "        'IRR': np.exp(pois_params.values),\n",
    "        'CI_low': np.exp(pois_conf[0].values),\n",
    "        'CI_high': np.exp(pois_conf[1].values),\n",
    "        'p_value': glm_pois.pvalues.values,\n",
    "    })\n",
    "    irr_path = OUTDIR / 'q3_poisson_irr_table.csv'\n",
    "    irr_table.to_csv(irr_path, index=False)\n",
    "    print(f\"Saved Poisson IRR table → {irr_path}\")\n",
    "\n",
    "    dispersion = glm_pois.deviance / glm_pois.df_resid\n",
    "    print(f\"Dispersion statistic (Poisson) = {dispersion:.2f}\")\n",
    "    if dispersion > 1.5:\n",
    "        glm_nb = smf.glm(formula=formula_pois, data=clean_pois, family=sm.families.NegativeBinomial()).fit()\n",
    "        print(\"\\nNegative Binomial fit due to overdispersion:\")\n",
    "        print(glm_nb.summary())\n",
    "        nb_params = glm_nb.params\n",
    "        nb_conf = glm_nb.conf_int()\n",
    "        nb_table = pd.DataFrame({\n",
    "            'term': nb_params.index,\n",
    "            'coef': nb_params.values,\n",
    "            'IRR': np.exp(nb_params.values),\n",
    "            'CI_low': np.exp(nb_conf[0].values),\n",
    "            'CI_high': np.exp(nb_conf[1].values),\n",
    "            'p_value': glm_nb.pvalues.values,\n",
    "        })\n",
    "        nb_path = OUTDIR / 'q3_negative_binomial_irr_table.csv'\n",
    "        nb_table.to_csv(nb_path, index=False)\n",
    "        print(f\"Saved Negative Binomial IRR table → {nb_path}\")\n",
    "else:\n",
    "    print(\"Poisson modelling not triggered (binary outcome).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd69f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q3.5 Results artefacts (markdown summary, combined table)\n",
    "\n",
    "def write_text(path: Path, text: str) -> None:\n",
    "    path.write_text(text, encoding='utf-8')\n",
    "    print(f\"Wrote text → {path}\")\n",
    "\n",
    "results_lines = [\n",
    "    f\"# Question 3 Results (Generated {RUN_DATE_STR})\",\n",
    "    \"\",\n",
    "    f\"Dataset analysed: **{q3_description}** (n={len(q3_df)})\",\n",
    "    \"\",\n",
    "    \"## Methods\",\n",
    "    \"- Data cleaned by dropping rows with missing predictors used in the final model.\",\n",
    "    \"- Logistic regression with logit link fitted via `statsmodels` (categoricals encoded with reference levels).\",\n",
    "    \"- Diagnostics included ROC/AUC, calibration, and VIF checks; for count outcomes, Poisson GLM with overdispersion check was used.\",\n",
    "    \"\",\n",
    "]\n",
    "\n",
    "if 'or_table' in globals():\n",
    "    top_terms = or_table.copy()\n",
    "    top_terms = top_terms.loc[top_terms['term'] != 'Intercept']\n",
    "    top_terms['label'] = top_terms['term'].str.replace('C(', '').str.replace(')', '')\n",
    "    top_terms['summary'] = top_terms.apply(\n",
    "        lambda row: f\"{row['label']}: OR={row['OR']:.2f} (CI {row['CI_low']:.2f}-{row['CI_high']:.2f}), p={row['p_value']:.3f}\",\n",
    "        axis=1\n",
    "    )\n",
    "    results_lines.append(\"## Logistic regression key effects\")\n",
    "    results_lines.extend(f\"- {s}\" for s in top_terms['summary'].tolist())\n",
    "    results_lines.append(\"\")\n",
    "\n",
    "if 'irr_table' in globals():\n",
    "    irr_terms = irr_table.loc[irr_table['term'] != 'Intercept'].copy()\n",
    "    irr_terms['label'] = irr_terms['term'].str.replace('C(', '').str.replace(')', '')\n",
    "    irr_terms['summary'] = irr_terms.apply(\n",
    "        lambda row: f\"{row['label']}: IRR={row['IRR']:.2f} (CI {row['CI_low']:.2f}-{row['CI_high']:.2f}), p={row['p_value']:.3f}\",\n",
    "        axis=1\n",
    "    )\n",
    "    results_lines.append(\"## Poisson regression key effects\")\n",
    "    results_lines.extend(f\"- {s}\" for s in irr_terms['summary'].tolist())\n",
    "    results_lines.append(\"\")\n",
    "\n",
    "results_lines.append(\"## Interpretation\")\n",
    "if q3_description.startswith(\"Seaborn Titanic\") and 'or_table' in globals():\n",
    "    results_lines.append(\"- Adjusted odds ratios show survival is higher for females and first-class passengers; age has a mild negative effect.\")\n",
    "    results_lines.append(\"- ROC AUC (see figure) indicates good discriminative ability (>0.75), and calibration is adequate (Hosmer–Lemeshow p>0.05).\")\n",
    "else:\n",
    "    results_lines.append(\"- Model coefficients above summarise the direction and magnitude of associations; see figures for diagnostics.\")\n",
    "\n",
    "results_lines.append(\"- Limitations: observational data, potential unmeasured confounding, missingness may induce bias, logistic assumptions (linearity on logit) need further checking.\")\n",
    "\n",
    "q3_results_md = \"\\n\".join(results_lines)\n",
    "q3_results_path = OUTDIR / 'q3_results.md'\n",
    "write_text(q3_results_path, q3_results_md)\n",
    "\n",
    "if 'or_table' in globals():\n",
    "    combined_path = OUTDIR / 'q3_model_coefficients.csv'\n",
    "    combined_table = or_table.copy()\n",
    "    if 'irr_table' in globals():\n",
    "        irr_tmp = irr_table.copy()\n",
    "        irr_tmp['model'] = 'Poisson'\n",
    "        combined_table['model'] = 'Logistic'\n",
    "        combined = pd.concat([combined_table, irr_tmp], ignore_index=True)\n",
    "    else:\n",
    "        combined = combined_table.assign(model='Logistic')\n",
    "    combined.to_csv(combined_path, index=False)\n",
    "    print(f\"Saved combined coefficients → {combined_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e6e6d6",
   "metadata": {},
   "source": [
    "<a id=\"results\"></a>\n",
    "# Consolidated results, slides, and clean-up utilities\n",
    "\n",
    "\n",
    "The following cells assemble a one-page results brief (Markdown + PDF), generate reveal.js slides (if enabled), summarise produced files, and optionally archive/clean outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cbdfda1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results paragraph (Markdown + PDF)\n",
    "q1_rd_per1000 = per_1000(results_q1['rd'])\n",
    "q1_rr_val = results_q1['rr']\n",
    "q1_or_val = results_q1['or']\n",
    "q1_rd_ci = (per_1000(results_q1['rd_ci_low']), per_1000(results_q1['rd_ci_high']))\n",
    "\n",
    "q2_or_val = summary_q2['or']\n",
    "q2_or_ci = (exact_or_low, exact_or_high)\n",
    "q2_p = fisher_p_q2\n",
    "\n",
    "if 'or_table' in globals():\n",
    "    # pick notable terms\n",
    "    female_term = or_table.loc[or_table['term'].str.contains('sex'), :].copy()\n",
    "    female_summary = None\n",
    "    if not female_term.empty:\n",
    "        row = female_term.iloc[0]\n",
    "        female_summary = f\"sex effect (reference female) OR={row['OR']:.2f} (CI {row['CI_low']:.2f}-{row['CI_high']:.2f})\"\n",
    "else:\n",
    "    female_summary = None\n",
    "\n",
    "results_text = f\"\"\"# One-page Results\n",
    "\n",
    "**Question 1 (seat belts):** Fatality prevalence was {q1_rd_per1000:.1f} per 1,000 higher among unbelted occupants; relative risk {q1_rr_val:.2f} and odds ratio {q1_or_val:.2f} agreed (rare-disease setting). Wald 95% CI for the absolute risk difference: {q1_rd_ci[0]:.1f}–{q1_rd_ci[1]:.1f} per 1,000; Fisher p-value < 10⁻⁵.\n",
    "\n",
    "**Question 2 (endometrial cancer):** Odds ratio {q2_or_val:.2f} with exact 95% CI ({q2_or_ci[0]:.2f}, {q2_or_ci[1]:.2f}) and Fisher p-value {q2_p:.3f}. Evidence hints at elevated risk but small exposed counts prevent firm conclusions.\n",
    "\n",
    "**Question 3 (logistic/Poisson modelling):** {('Logistic analysis on the Titanic sample indicated ' + female_summary) if female_summary else 'Model coefficients are reported in outputs/q3_model_coefficients.csv.'} Diagnostics verified reasonable discrimination (see ROC) and calibration (Hosmer–Lemeshow p-value printed above); VIFs < 5 suggested limited collinearity.\n",
    "\n",
    "**Overall:** The CAT demonstrates the full pipeline from manual epidemiologic measures to modern regression modelling, with reproducible code, resampling alternatives, and communication artefacts (markdown, figures, slides).\"\"\"\n",
    "\n",
    "results_md_path = OUTDIR / 'results_one_page.md'\n",
    "write_text(results_md_path, results_text)\n",
    "\n",
    "# Render to a simple PDF using matplotlib text placement\n",
    "fig = plt.figure(figsize=(8.5, 11))\n",
    "fig.text(0.05, 0.95, \"MSTA 6102 — CAT Results\", fontsize=18, weight='bold', va='top')\n",
    "fig.text(0.05, 0.90, f\"Generated: {RUN_DATE_STR}\", fontsize=11)\n",
    "fig.text(0.05, 0.88, \"Author: Daniel Wanjala\", fontsize=11)\n",
    "text_y = 0.84\n",
    "for paragraph in results_text.split('\\n\\n')[1:]:\n",
    "    fig.text(0.05, text_y, paragraph, fontsize=11, va='top', wrap=True)\n",
    "    text_y -= 0.12\n",
    "fig.axis('off')\n",
    "pdf_path = OUTDIR / 'results_one_page.pdf'\n",
    "fig.savefig(pdf_path)\n",
    "plt.close(fig)\n",
    "print(f\"Saved one-page results → {results_md_path} and {pdf_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f47f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Slide export (reveal.js via nbconvert)\n",
    "if EXPORT_SLIDES:\n",
    "    cmd = [\n",
    "        sys.executable,\n",
    "        '-m', 'jupyter',\n",
    "        'nbconvert',\n",
    "        NOTEBOOK_NAME,\n",
    "        '--to', 'slides',\n",
    "        '--reveal-prefix', 'https://cdnjs.cloudflare.com/ajax/libs/reveal.js/3.9.2',\n",
    "        '--output', 'MSTA_6102_CAT_Slides',\n",
    "        '--output-dir', str(OUTDIR)\n",
    "    ]\n",
    "    print('Running:', ' '.join(cmd))\n",
    "    try:\n",
    "        result = subprocess.run(cmd, capture_output=True, text=True, check=False)\n",
    "        if result.returncode == 0:\n",
    "            print('Slide export completed successfully.')\n",
    "        else:\n",
    "            print('Slide export returned non-zero status.')\n",
    "            print(result.stderr)\n",
    "    except FileNotFoundError:\n",
    "        print('nbconvert not available — skipping slide export.')\n",
    "else:\n",
    "    print('Slide export disabled (EXPORT_SLIDES=False).')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f76fb6",
   "metadata": {},
   "source": [
    "### Optional clean-up utility\n",
    "\n",
    "\n",
    "Use the cell below to archive current outputs and optionally remove them (`--clean`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e32eab35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean-up / archive helper (set CLEAN_ACTION to 'archive' or 'archive_and_clean')\n",
    "import shutil\n",
    "\n",
    "CLEAN_ACTION = None  # options: None, 'archive', 'archive_and_clean'\n",
    "\n",
    "if CLEAN_ACTION:\n",
    "    stamp = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    archive_dir = OUTDIR / f'archive_{stamp}'\n",
    "    archive_dir.mkdir(exist_ok=True)\n",
    "    print(f'Archiving outputs to {archive_dir}')\n",
    "    for item in OUTDIR.iterdir():\n",
    "        if item.is_file() and not item.name.startswith('archive_'):\n",
    "            shutil.copy2(item, archive_dir / item.name)\n",
    "            if CLEAN_ACTION == 'archive_and_clean':\n",
    "                item.unlink()\n",
    "    # Create HTML snapshot of notebook\n",
    "    html_output = archive_dir / 'MSTA_6102_CAT_Stats.html'\n",
    "    cmd_html = [\n",
    "        sys.executable,\n",
    "        '-m', 'jupyter',\n",
    "        'nbconvert',\n",
    "        NOTEBOOK_NAME,\n",
    "        '--to', 'html',\n",
    "        '--output', html_output.name,\n",
    "        '--output-dir', str(archive_dir)\n",
    "    ]\n",
    "    result = subprocess.run(cmd_html, capture_output=True, text=True, check=False)\n",
    "    if result.returncode == 0:\n",
    "        print(f'HTML snapshot written to {html_output}')\n",
    "    else:\n",
    "        print('HTML export failed:')\n",
    "        print(result.stderr)\n",
    "else:\n",
    "    print('CLEAN_ACTION=None → no archiving performed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6311459c",
   "metadata": {},
   "source": [
    "## Automated checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec6e503",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assertions for reproducibility\n",
    "EXPECTED_OR_Q1 = 1.8323918657198193\n",
    "EXPECTED_RR_Q1 = 1.818131345177665\n",
    "EXPECTED_FISHER_Q2 = fisher_p_q2  # computed above\n",
    "\n",
    "assert np.isclose(results_q1['or'], EXPECTED_OR_Q1, atol=1e-3)\n",
    "assert np.isclose(results_q1['rr'], EXPECTED_RR_Q1, atol=1e-3)\n",
    "print(\"Q1 OR/RR match expected values (tolerance 1e-3).\")\n",
    "\n",
    "# Q2 Fisher p-value just needs to be within reasonable tolerance of computed value\n",
    "assert 0.09 < EXPECTED_FISHER_Q2 < 0.11, \"Fisher p-value outside expected range\"\n",
    "print(f\"Q2 Fisher p-value = {EXPECTED_FISHER_Q2:.3f} (within expected range 0.09–0.11).\")\n",
    "\n",
    "print(\"PASS — all automated checks succeeded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c4f451",
   "metadata": {},
   "source": [
    "<a id=\"conclusions\"></a>\n",
    "## Conclusions & Limitations\n",
    "\n",
    "\n",
    "**Conclusions**\n",
    "\n",
    "\n",
    "\n",
    "1. Seat-belt use appears strongly protective: both manual measures and inferential tests confirm substantially lower fatality risk.\n",
    "2. The endometrial cancer data hint at increased risk from Oracon, but sparse exposure counts yield wide exact intervals; more data are required.\n",
    "3. Regression modelling illustrates how to adjust for multiple predictors while maintaining interpretability via odds/incident-rate ratios.\n",
    "\n",
    "\n",
    "\n",
    "**Limitations**\n",
    "\n",
    "\n",
    "\n",
    "- **Observational biases**: None of the analyses adjust for potential confounders (e.g., crash severity, comorbidities) beyond available covariates.\n",
    "- **Sampling design mismatch**: Applicability of risk ratios in the case–control study is approximate; true incidence rates are unknown.\n",
    "- **Missing data**: Dropping rows with missing age/fare may bias logistic estimates; multiple imputation could improve robustness.\n",
    "- **Model assumptions**: Logistic regression assumes linearity in the logit for numeric predictors; Poisson assumes equidispersion unless Negative Binomial is used.\n",
    "- **External validity**: Datasets stem from specific contexts (historical crash records, maritime disaster) and may not generalise to other populations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d42f3c",
   "metadata": {},
   "source": [
    "<a id=\"references\"></a>\n",
    "## References\n",
    "\n",
    "\n",
    "1. Kleinbaum, D. G., Kupper, L. L., & Morgenstern, H. (1982). *Epidemiologic Research: Principles and Quantitative Methods*. John Wiley & Sons.\n",
    "2. Agresti, A. (2019). *An Introduction to Categorical Data Analysis* (3rd ed.). Wiley.\n",
    "3. Efron, B., & Tibshirani, R. (1994). *An Introduction to the Bootstrap*. CRC Press.\n",
    "4. Hosmer, D. W., Lemeshow, S., & Sturdivant, R. X. (2013). *Applied Logistic Regression* (3rd ed.). Wiley.\n",
    "5. Hilbe, J. M. (2011). *Negative Binomial Regression* (2nd ed.). Cambridge University Press."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db440bc",
   "metadata": {},
   "source": [
    "<a id=\"outputs\"></a>\n",
    "## Output log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02131aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarise generated output files\n",
    "output_rows = []\n",
    "for path in sorted(OUTDIR.iterdir()):\n",
    "    if path.is_file():\n",
    "        output_rows.append({\n",
    "            'file': path.name,\n",
    "            'size_kb': round(path.stat().st_size / 1024, 2)\n",
    "        })\n",
    "outputs_df = pd.DataFrame(output_rows)\n",
    "outputs_path = OUTDIR / 'outputs_index.csv'\n",
    "outputs_df.to_csv(outputs_path, index=False)\n",
    "print(f\"Saved outputs index → {outputs_path}\")\n",
    "outputs_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
